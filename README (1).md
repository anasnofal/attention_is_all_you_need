# ARâ€“EN Transformer (PyTorch)
Implementation of a Transformer-based neural machine translation (NMT) model inspired by **â€œAttention Is All You Needâ€ (Vaswani et al., 2017)**, trained on **custom parallel data** for **Arabic â†” English**. The project focuses on **lowâ€‘resource Arabic** scenarios and demonstrates practical techniques for data preparation, training, and evaluation in PyTorch.

> Notebook: `Text-Translation-Transformer.ipynb`

---

## âœ¨ What this repo contains
- A clean, fromâ€‘scratch **Transformer** in **PyTorch** for sequenceâ€‘toâ€‘sequence translation.
- **Custom-data** training pipeline for **Arabic â†” English (ARâ€“EN)**.
- Lowâ€‘resource strategies (shared BPE, label smoothing, subword regularization, early stopping).
- Reproducible evaluation (SacreBLEU) and simple inference utilities.

---

## ğŸ§  Background
This implementation follows the core ideas from the Transformer architecture introduced in *Attention Is All You Need*. No RNNs or CNNs â€” just multiâ€‘head selfâ€‘attention, positional encodings, and feedâ€‘forward layers in an encoderâ€“decoder arrangement.

Key components:
- Token + positional embeddings
- Multiâ€‘Head Selfâ€‘Attention (encoder) and Crossâ€‘Attention (decoder)
- Positionâ€‘wise feedâ€‘forward networks
- Residual connections & LayerNorm
- Teacher forcing during training and autoregressive decoding at inference

---

## ğŸ“‚ Project structure
```
.
â”œâ”€â”€ Text-Translation-Transformer.ipynb   # End-to-end training & evaluation
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.ar  train.en               # Your parallel training files
â”‚   â”œâ”€â”€ valid.ar  valid.en               # Validation files
â”‚   â””â”€â”€ test.ar   test.en                # Test files
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ spm.model  spm.vocab             # SentencePiece model (shared)
â”‚   â”œâ”€â”€ vocab.json                       # Token â†’ id mapping (optional)
â”‚   â””â”€â”€ checkpoints/                     # Saved PyTorch checkpoints
â””â”€â”€ README.md
```
> Adjust paths if your dataset layout differs.

---

## ğŸ› ï¸ Setup
Tested with Python 3.10+.

```bash
# 1) Create environment
python -m venv .venv
source .venv/bin/activate  # on Windows: .venv\Scripts\activate

# 2) Install dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # or cpu
pip install sentencepiece sacrebleu torchmetrics tqdm pyyaml
```

**Optional (nice to have):**
```bash
pip install rich matplotlib
```

---

## ğŸ—‚ï¸ Data preparation
1. **Normalize & clean Arabic/English text** (Unicode NFKC, remove control chars, strip punctuation where appropriate).
2. **Train shared subword model** with SentencePiece (BPE or Unigram):
   ```bash
   spm_train      --input=data/train.ar,data/train.en      --model_prefix=artifacts/spm      --vocab_size=16000      --character_coverage=1.0      --model_type=bpe
   ```
3. **Encode splits**:
   ```bash
   spm_encode --model=artifacts/spm.model --output_format=id      < data/train.ar > data/train.ar.ids
   spm_encode --model=artifacts/spm.model --output_format=id      < data/train.en > data/train.en.ids
   # repeat for valid/test
   ```

**Lowâ€‘resource tips (Arabic):**
- Use **shared BPE** across AR/EN to increase lexical sharing.
- Enable **subword regularization** (`--sample_piece`) during training for robustness.
- Apply **label smoothing** (e.g., Îµ = 0.1) and **dropout** (e.g., 0.1â€“0.3).
- **Early stopping** on SacreBLEU or validation loss.
- If you have monolingual data, consider **backâ€‘translation** to augment pairs.

---

## ğŸš€ Training (from the notebook)
Open `Text-Translation-Transformer.ipynb` and run the cells in order. The notebook includes:
- Dataset loading & tokenization (SentencePiece)
- Model definition (Encoder, Decoder, Generator)
- Training loop with Adam/AdamW, warmup schedule (Noam), mixed precision
- Checkpointing & early stopping
- Evaluation with SacreBLEU

**Typical hyperparameters (good starting point):**
- `d_model=512`, `n_heads=8`, `num_layers=6`
- `ffn_dim=2048`, `dropout=0.1`
- `bpe_vocab_size=16k` (tune 8kâ€“32k)
- `batch_size=4096` tokens (dynamic batching by length is recommended)
- `lr=5e-4` with **warmup** (e.g., 4000 steps), **label_smoothing=0.1`

---

## ğŸ” Evaluation
Use **SacreBLEU** for standardized BLEU:
```python
from sacrebleu import corpus_bleu
refs = [["the reference translation one", "another reference"]]
hyps = ["the system translation one"]
print(corpus_bleu(hyps, refs).format())
```
For Arabic, also inspect **chrF** and **COMET** if available. Manual spotâ€‘checks are valuable due to morphology and diacritics.

---

## ğŸ’¬ Inference
Greedy or beam search decoding (beam=4â€“8) with length penalty often helps.

```python
from pathlib import Path
import sentencepiece as spm
import torch

# Load
sp = spm.SentencePieceProcessor(model_file="artifacts/spm.model")
model = torch.load("artifacts/checkpoints/best.pt", map_location="cpu")
model.eval()

def translate_ar_to_en(text: str, max_len=128, beam=5):
    ids = sp.encode(text, out_type=int)
    # ... pass through encoder; run beam search in decoder ...
    # return detokenized string
```

> The notebook shows a complete example of encoding, decoding, and detokenization.

---

## ğŸ“ˆ Reproducibility
- Set `torch.manual_seed(42)` and deterministic flags where feasible.
- Log config & metrics to a JSON/YAML file per run.
- Save the **SentencePiece model**, **checkpoint**, and **commit hash** for each experiment.

---

## ğŸ§ª Known limitations
- Truecasing/diacritics in Arabic are not modeled explicitly.
- Domain mismatch can degrade performance; consider domainâ€‘adaptive fineâ€‘tuning.
- For very low resources, backâ€‘translation and multilingual transfer can be decisive.

---

## ğŸ§¯ Ethical use
Machine translation can produce errors that alter meaning. Do not rely on this model for highâ€‘stakes contexts (legal, medical, safetyâ€‘critical) without human review. Be mindful of biases in training data.

---

## ğŸ“š Reference
- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Åukasz Kaiser, and Illia Polosukhin. *Attention Is All You Need.* NeurIPS 2017.

---

## ğŸ“ Citation
If you use this code/notebook in academic work, please cite the original paper above and this repository/notebook as appropriate.

---

## ğŸ“„ License
Specify your license (e.g., MIT) here.
