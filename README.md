# ARâ€“EN Transformer (PyTorch)
Implementation of a Transformer-based neural machine translation (NMT) model inspired by **â€œAttention Is All You Needâ€ (Vaswani et al., 2017)**, trained on **custom parallel data** for **Arabic â†” English**. The project focuses on **lowâ€‘resource Arabic** scenarios and demonstrates practical techniques for data preparation, training, and evaluation in PyTorch.

> Notebook: `Text-Translation-Transformer.ipynb`

---

## âœ¨ What this repo contains
- A clean, fromâ€‘scratch **Transformer** in **PyTorch** for sequenceâ€‘toâ€‘sequence translation.
- **Custom-data** training pipeline for **Arabic â†” English (ARâ€“EN)**.
- Lowâ€‘resource strategies (shared BPE, label smoothing, subword regularization, early stopping).
- Reproducible evaluation (SacreBLEU) and simple inference utilities.

---

## ğŸ§  Background
This implementation follows the core ideas from the Transformer architecture introduced in *Attention Is All You Need*. No RNNs or CNNs â€” just multiâ€‘head selfâ€‘attention, positional encodings, and feedâ€‘forward layers in an encoderâ€“decoder arrangement.

Key components:
- Token + positional embeddings
- Multiâ€‘Head Selfâ€‘Attention (encoder) and Crossâ€‘Attention (decoder)
- Positionâ€‘wise feedâ€‘forward networks
- Residual connections & LayerNorm
- Teacher forcing during training and autoregressive decoding at inference

```
   ```

**Lowâ€‘resource tips (Arabic):**
- Use **shared BPE** across AR/EN to increase lexical sharing.
- Enable **subword regularization** (`--sample_piece`) during training for robustness.
- Apply **label smoothing** (e.g., Îµ = 0.1) and **dropout** (e.g., 0.1â€“0.3).
- **Early stopping** on SacreBLEU or validation loss.
- If you have monolingual data, consider **backâ€‘translation** to augment pairs.

---





## ğŸ“š Reference
- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Åukasz Kaiser, and Illia Polosukhin. *Attention Is All You Need.* NeurIPS 2017.

---


---

## ğŸ“„ License
Specify your license (e.g., MIT) here.
